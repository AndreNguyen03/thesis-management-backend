import { BadRequestException, Inject, Injectable } from '@nestjs/common'
import { InjectQueue } from '@nestjs/bull'
import { Queue } from 'bullmq'
import { BuildKnowledgeDB } from '../dtos/build-knowledge-db.dto'
import { CreateKnowledgeSourceProvider } from '../../knowledge-source/application/create-knowledge-source.provider'
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters'
import { PuppeteerWebBaseLoader } from '@langchain/community/document_loaders/web/puppeteer'
import { GetEmbeddingProvider } from './get-embedding.provider'
import { CreateKnowledgeChunksProvider } from '../../knowledge-source/application/create-knowledge-chunks.provider'
import { SourceType } from '../../knowledge-source/enums/source_type.enum'
import { KnowledgeChunk } from '../../knowledge-source/schemas/knowledge-chunk.schema'
import { KnowledgeSource } from '../../knowledge-source/schemas/knowledge-source.schema'
import {
    SearchOptions,
    SearchSimilarDocumentsProvider
} from '../../knowledge-source/application/search-similar-documents.provider'
import { UploadManyFilesProvider } from '../../upload-files/providers/upload-many-files.provider'
import { PDFLoader } from '@langchain/community/document_loaders/fs/pdf'
import * as pdfjsLib from 'pdfjs-dist/legacy/build/pdf.mjs'
import { Canvas, createCanvas, Image } from 'canvas'
import * as fs from 'fs'
import * as path from 'path'
import * as os from 'os'
import * as pdfPoppler from 'pdf-poppler'
import * as Tesseract from 'tesseract.js'
import { UploadFileTypes } from '../../upload-files/enum/upload-files.type.enum'
import { CreateKnowledgeChunkDto } from '../../knowledge-source/dto/create-knowledge-chunk.dto'
import { ChatbotGateway } from '../gateways/chatbot.gateway'
import { ProcessingStatus } from '../../knowledge-source/enums/processing-status.enum'

// NodeCanvasFactory ƒë·ªÉ l√†m c·∫ßu n·ªëi gi·ªØa pdfjs v√† node-canvas

@Injectable()
export class RetrievalProvider {
    constructor(
        private readonly getEmbeddingProvider: GetEmbeddingProvider,
        private readonly knowledgeChunksProvider: CreateKnowledgeChunksProvider,
        private readonly knowledgeSourceProvider: CreateKnowledgeSourceProvider,
        private readonly searchSimilarDocumentsProvider: SearchSimilarDocumentsProvider,
        @InjectQueue('knowledge-processing') private readonly knowledgeQueue: Queue,
        private readonly uploadManyFilesProvider: UploadManyFilesProvider,
        private readonly chatbotGateway: ChatbotGateway
    ) {}

    /**
     * Ph√°t hi·ªán lo·∫°i PDF: text-based ho·∫∑c scanned
     */
    private async detectPDFType(pdfBuffer: Buffer): Promise<'text' | 'scanned'> {
        const pdf = await pdfjsLib.getDocument({ data: new Uint8Array(pdfBuffer) }).promise
        const page = await pdf.getPage(1)
        const textContent = await page.getTextContent()

        // N·∫øu c√≥ √≠t h∆°n 25 k√Ω t·ª± trong trang ƒë·∫ßu ‚Üí PDF scanned
        return textContent.items.length < 10 ? 'scanned' : 'text'
    }

    /**
     * X·ª≠ l√Ω PDF text-based b·∫±ng PDFLoader
     */
    private async extractTextFromPDF(fileBuffer: Buffer): Promise<string> {
        // Convert Node.js Buffer to Uint8Array for Blob compatibility
        const loader = new PDFLoader(new Blob([new Uint8Array(fileBuffer)]))
        const docs = await loader.load()
        return docs.map((doc) => doc.pageContent).join('\n\n')
    }

    /**
     * X·ª≠ l√Ω PDF scanned b·∫±ng OCR (Tesseract)
     * Uses pdf-poppler to convert PDF to images and tesseract.js for OCR
     */
    async extractFromScannedPdf(buffer: Buffer): Promise<string> {
        // S·ª≠ d·ª•ng system temp directory ƒë·ªÉ tr√°nh v·∫•n ƒë·ªÅ v·ªõi Unicode trong ƒë∆∞·ªùng d·∫´n
        const tempDir = path.join(os.tmpdir(), 'pdf-ocr', Date.now().toString())

        try {
            // T·∫°o th∆∞ m·ª•c t·∫°m
            fs.mkdirSync(tempDir, { recursive: true })

            // L∆∞u PDF v√†o file t·∫°m
            const pdfPath = path.join(tempDir, 'input.pdf')
            fs.writeFileSync(pdfPath, buffer)

            // C·∫•u h√¨nh pdf-poppler ƒë·ªÉ convert PDF sang PNG
            const options = {
                format: 'png',
                out_dir: tempDir,
                out_prefix: 'page',
                page: null // Convert all pages
            }

            // Convert PDF th√†nh images
            await pdfPoppler.convert(pdfPath, options)

            // Kh·ªüi t·∫°o Tesseract worker v·ªõi ti·∫øng Vi·ªát v√† ti·∫øng Anh
            const worker = await Tesseract.createWorker(['vie', 'eng'])
            let extractedText = ''

            // ƒê·ªçc t·∫•t c·∫£ file PNG ƒë∆∞·ª£c t·∫°o ra
            const imageFiles = fs
                .readdirSync(tempDir)
                .filter((file) => file.endsWith('.png'))
                .sort() // S·∫Øp x·∫øp theo th·ª© t·ª± trang

            console.log(`Found ${imageFiles.length} pages to process`)

            // OCR t·ª´ng ·∫£nh
            for (const imageFile of imageFiles) {
                const imagePath = path.join(tempDir, imageFile)
                console.log(`Processing: ${imageFile}`)

                const { data } = await worker.recognize(imagePath)
                extractedText += data.text + '\n\n'
            }

            // D·ªçn d·∫πp
            await worker.terminate()

            return extractedText.trim()
        } catch (error) {
            console.error('Error in extractFromScannedPdf:', error)
            throw new BadRequestException(`Failed to extract text from scanned PDF: ${error.message}`)
        } finally {
            // ƒê·∫£m b·∫£o x√≥a th∆∞ m·ª•c t·∫°m d√π c√≥ l·ªói hay kh√¥ng
            if (fs.existsSync(tempDir)) {
                fs.rmSync(tempDir, { recursive: true, force: true })
            }
        }
    }

    /**
     * Tr√≠ch xu·∫•t, ph√¢n m·∫£nh, embedding v√† l∆∞u tr·ªØ knowledge t·ª´ file upload
     */
    private async processFileToChunks(file: Express.Multer.File, knowledgeSourceIdLocal: string): Promise<void> {
        try {
            // L·∫•y knowledge source ƒë·ªÉ bi·∫øt source_type
            const knowledgeSource = await this.knowledgeSourceProvider.getKnowledgeSourceById(knowledgeSourceIdLocal)
            if (!knowledgeSource) {
                throw new BadRequestException('Knowledge source not found')
            }

            // Emit crawling progress
            this.chatbotGateway.emitCrawlProgress({
                resourceId: knowledgeSourceIdLocal,
                status: 'crawling',
                progress: 10,
                message: 'ƒêang tr√≠ch xu·∫•t n·ªôi dung t·ª´ file...'
            })

            // C·∫≠p nh·∫≠t processing status
            await this.knowledgeSourceProvider.updateKnowledgeSourceStatus(
                knowledgeSourceIdLocal,
                ProcessingStatus.PENDING
            )

            let extractedText = ''

            if (file.mimetype === 'application/pdf') {
                const pdfType = await this.detectPDFType(file.buffer)
                console.log('Detected PDF type:', pdfType)

                this.chatbotGateway.emitCrawlProgress({
                    resourceId: knowledgeSourceIdLocal,
                    status: 'crawling',
                    progress: 30,
                    message: `ƒêang x·ª≠ l√Ω PDF ${pdfType}...`
                })

                if (pdfType === 'text') {
                    extractedText = await this.extractTextFromPDF(file.buffer)
                } else {
                    extractedText = await this.extractFromScannedPdf(file.buffer)
                }
            } else if (file.mimetype.startsWith('image/')) {
                // X·ª≠ l√Ω ·∫£nh tr·ª±c ti·∫øp
                this.chatbotGateway.emitCrawlProgress({
                    resourceId: knowledgeSourceIdLocal,
                    status: 'crawling',
                    progress: 30,
                    message: 'ƒêang nh·∫≠n d·∫°ng OCR cho ·∫£nh...'
                })

                const worker = await Tesseract.createWorker('vie+eng')
                const { data } = await worker.recognize(file.buffer)
                await worker.terminate()
                extractedText = data.text
            }

            console.log('Extracted Text:', extractedText.slice(0, 500))
            const wordCount = extractedText.split(/\s+/).length

            this.chatbotGateway.emitCrawlProgress({
                resourceId: knowledgeSourceIdLocal,
                status: 'crawling',
                progress: 50,
                message: `ƒê√£ tr√≠ch xu·∫•t ${wordCount} t·ª´. ƒêang ph√¢n chia chunks...`
            })

            // 4. Split text th√†nh chunks
            const splitter = new RecursiveCharacterTextSplitter({
                chunkSize: 1024,
                chunkOverlap: 100,
                separators: ['\n\n', '\n', '. ']
            })
            const chunks = await splitter.splitText(extractedText)

            this.chatbotGateway.emitCrawlCompleted({
                resourceId: knowledgeSourceIdLocal,
                status: 'completed',
                progress: 60,
                message: `ƒê√£ t·∫°o ${chunks.length} chunks. B·∫Øt ƒë·∫ßu embedding...`
            })

            // 5. Embedding v√† l∆∞u chunks (batch processing)
            this.chatbotGateway.emitEmbeddingProgress({
                resourceId: knowledgeSourceIdLocal,
                status: 'embedding',
                progress: 60,
                message: `ƒêang t·∫°o embedding cho ${chunks.length} chunks...`
            })

            const knowledgeChunks: CreateKnowledgeChunkDto[] = []
            let processedChunks = 0

            for (const chunk of chunks) {
                const vector = await this.getEmbeddingProvider.getEmbedding(chunk)
                knowledgeChunks.push({
                    source_id: knowledgeSourceIdLocal,
                    source_type: knowledgeSource.source_type, // L∆∞u source_type v√†o chunk
                    text: chunk,
                    plot_embedding_gemini_large: vector
                })

                processedChunks++
                const embeddingProgress = 60 + (processedChunks / chunks.length) * 30

                if (processedChunks % 5 === 0 || processedChunks === chunks.length) {
                    this.chatbotGateway.emitEmbeddingProgress({
                        resourceId: knowledgeSourceIdLocal,
                        status: 'embedding',
                        progress: Math.round(embeddingProgress),
                        message: `ƒê√£ embedding ${processedChunks}/${chunks.length} chunks...`
                    })
                }
            }

            // L∆∞u t·∫•t c·∫£ chunks c√πng l√∫c
            await this.knowledgeChunksProvider.insertKnowledgeChunks(knowledgeChunks)

            // C·∫≠p nh·∫≠t metadata
            await this.knowledgeSourceProvider.updateKnowledgeSourceMetadata(knowledgeSourceIdLocal, {
                wordCount,
                chunkCount: chunks.length,
                fileSize: file.size,
                mimeType: file.mimetype,
                progress: 100
            })

            await this.knowledgeSourceProvider.updateKnowledgeSourceStatus(
                knowledgeSourceIdLocal,
                ProcessingStatus.COMPLETED
            )

            this.chatbotGateway.emitEmbeddingCompleted({
                resourceId: knowledgeSourceIdLocal,
                status: 'completed',
                progress: 100,
                message: `Ho√†n th√†nh! ƒê√£ t·∫°o ${chunks.length} chunks t·ª´ ${wordCount} t·ª´.`
            })

            console.log(`Created ${chunks.length} chunks for knowledge source`)
        } catch (error) {
            console.error('Error processing file:', error)

            await this.knowledgeSourceProvider.updateKnowledgeSourceStatus(
                knowledgeSourceIdLocal,
                ProcessingStatus.FAILED
            )

            await this.knowledgeSourceProvider.updateKnowledgeSourceMetadata(knowledgeSourceIdLocal, {
                errorMessage: error.message
            })

            this.chatbotGateway.emitCrawlFailed({
                resourceId: knowledgeSourceIdLocal,
                status: 'failed',
                progress: 0,
                message: 'X·ª≠ l√Ω file th·∫•t b·∫°i',
                error: error.message
            })

            throw error
        }
    }
    /**
     * X·ª≠ l√Ω file upload ƒë·ªÉ t·∫°o knowledge source m·ªõi
     */
    public async processUploadedFileInNewKnowledge(
        userId: string,
        file: Express.Multer.File,
        sourceMetadata: { name: string; description?: string }
    ): Promise<KnowledgeSource> {
        // 1. Upload file l√™n MinIO
        const files = await this.uploadManyFilesProvider.uploadManyFiles(userId, [file], UploadFileTypes.AI_KNOWLEDGE)

        // 2. T·∫°o knowledge source v·ªõi status PENDING
        const newKnowledgeSource = await this.knowledgeSourceProvider.createKnowledgeSource(userId, {
            name: sourceMetadata.name,
            description: sourceMetadata.description,
            source_type: SourceType.FILE,
            source_location: files[0].fileUrl
        })

        // 3. X·ª≠ l√Ω file async (kh√¥ng ch·ªù ƒë·ª£i ƒë·ªÉ response nhanh cho client)
        this.processFileToChunks(file, newKnowledgeSource._id.toString()).catch((error) => {
            console.error('Error processing file asynchronously:', error)
        })

        return newKnowledgeSource
    }

    /**
     * X·ª≠ l√Ω file upload cho knowledge source ƒë√£ c√≥ s·∫µn
     * - Upload file l√™n MinIO
     * - X·ª≠ l√Ω file ƒë·ªÉ t·∫°o chunks
     * - L∆∞u chunks v√†o knowledge source ƒë√£ c√≥
     */
    public async processUploadedFileInAvailableKnowledge(
        userId: string,
        file: Express.Multer.File,
        klid: string
    ): Promise<KnowledgeSource> {
        // 1. Ki·ªÉm tra knowledge source t·ªìn t·∫°i
        const existingKnowledgeSource = await this.knowledgeSourceProvider.getKnowledgeSourceById(klid)
        if (!existingKnowledgeSource) {
            throw new BadRequestException('Knowledge source kh√¥ng t·ªìn t·∫°i')
        }

        // 2. Upload file l√™n MinIO v√† c·∫≠p nh·∫≠t source_location
        const uploadedFiles = await this.uploadManyFilesProvider.uploadManyFiles(
            userId,
            [file],
            UploadFileTypes.AI_KNOWLEDGE
        )
        await this.knowledgeSourceProvider.updateKnowledgeSourceLocation(klid, uploadedFiles[0].fileUrl)

        // 3. X·ª≠ l√Ω file async (kh√¥ng ch·ªù ƒë·ª£i)
        this.processFileToChunks(file, klid).catch((error) => {
            console.error('Error processing file asynchronously:', error)
        })

        return existingKnowledgeSource
    }

    public async searchSimilarDocuments(vectorSearch: number[], options: SearchOptions): Promise<KnowledgeChunk[]> {
        // Search similar documents in the database
        return await this.searchSimilarDocumentsProvider.searchSimilarDocuments(vectorSearch, options)
    }

    public async buildKnowledgeDocuments(
        userId: string,
        buildKnowledgeDB: BuildKnowledgeDB
    ): Promise<KnowledgeSource[]> {
        const knowledgeSources: KnowledgeSource[] = []
        for (const doc of buildKnowledgeDB.knowledgeDocuments) {
            //Store knowledge source metadata into database incase of URL type
            if (doc.source_type === SourceType.URL) {
                // Add knowledge documents to database
                const storedKnowledgeDoc = await this.knowledgeSourceProvider.createKnowledgeSource(userId, doc)
                //B·∫Øt ƒë·∫ßu x·ª≠ l√Ω d·ªØ li·ªáu crawl-split-embed-store cho t·ª´ng doc
                await this.loadSampleData(storedKnowledgeDoc._id.toString(), doc.source_location)
                // G·ª≠i job v√†o BullMQ
                // await this.knowledgeQueue.add('processKnowledge', {
                //     sourceId: storedKnowledgeDoc._id.toString(),
                //     url: doc.source_location
                // })
                // knowledgeSources.push(storedKnowledgeDoc)
            }
        }
        console.log('Completed building basic knowledge documents.')
        return knowledgeSources
    }

    public async loadSampleData(sourceId: string, url: string) {
        //Declare splitter to split text into chunks
        const splitter = new RecursiveCharacterTextSplitter({
            chunkSize: 1024,
            chunkOverlap: 100,
            separators: ['\n\n', '\n', '. ']
        })
        //create vector search if not exists

        try {
            // L·∫•y knowledge source ƒë·ªÉ bi·∫øt source_type
            const knowledgeSource = await this.knowledgeSourceProvider.getKnowledgeSourceById(sourceId)
            if (!knowledgeSource) {
                throw new BadRequestException('Knowledge source not found')
            }

            console.log(url)
            const content = await this.scrapePage(url)
            console.log('Scraped content length:', content)
            const chunks = await splitter.splitText(content)

            // Batch processing: Thu th·∫≠p t·∫•t c·∫£ embeddings tr∆∞·ªõc
            const knowledgeChunks: CreateKnowledgeChunkDto[] = []
            for (const chunk of chunks) {
                // Create embedding using Google
                const vector = await this.getEmbeddingProvider.getEmbedding(chunk)
                console.log('Created embedding for chunk', vector.length)
                knowledgeChunks.push({
                    source_id: sourceId,
                    source_type: knowledgeSource.source_type, // L∆∞u source_type
                    text: chunk,
                    plot_embedding_gemini_large: vector
                })
            }

            // L∆∞u t·∫•t c·∫£ chunks c√πng l√∫c
            const res = await this.knowledgeChunksProvider.createKnowledgeChunks(knowledgeChunks)
            console.log(`Successfully stored ${knowledgeChunks.length} chunks`)
            console.log('Batch insert result:', res)
        } catch (error) {
            throw new BadRequestException(error.message)
        }
    }

    /**
     * Crawl URL with progress tracking via WebSocket
     */
    public async loadSampleDataWithProgress(sourceId: string, url: string): Promise<void> {
        console.log(`üöÄ Starting loadSampleDataWithProgress for sourceId: ${sourceId}, url: ${url}`)

        const splitter = new RecursiveCharacterTextSplitter({
            chunkSize: 1024,
            chunkOverlap: 100,
            separators: ['\n\n', '\n', '. ']
        })

        try {
            // L·∫•y knowledge source ƒë·ªÉ bi·∫øt source_type
            const knowledgeSource = await this.knowledgeSourceProvider.getKnowledgeSourceById(sourceId)
            if (!knowledgeSource) {
                throw new BadRequestException('Knowledge source not found')
            }

            console.log('üì° Emitting crawl progress...')
            // Emit crawling started
            this.chatbotGateway.emitCrawlProgress({
                resourceId: sourceId,
                status: 'crawling',
                progress: 10,
                message: 'ƒêang crawl n·ªôi dung t·ª´ URL...'
            })

            console.log('üîÑ Updating knowledge source status to PENDING...')
            // Update status to PENDING
            await this.knowledgeSourceProvider.updateKnowledgeSourceStatus(sourceId, ProcessingStatus.PENDING)

            console.log('üåê Scraping page...')
            // Scrape page
            const content = await this.scrapePage(url)
            console.log(`‚úÖ Scraped ${content.length} characters from URL`)
            const wordCount = content.split(/\s+/).length

            this.chatbotGateway.emitCrawlProgress({
                resourceId: sourceId,
                status: 'crawling',
                progress: 40,
                message: `ƒê√£ crawl ${wordCount} t·ª´. ƒêang ph√¢n chia chunks...`
            })

            console.log('‚úÇÔ∏è Splitting into chunks...')
            // Split into chunks
            const chunks = await splitter.splitText(content)
            console.log(`‚úÖ Created ${chunks.length} chunks`)

            this.chatbotGateway.emitCrawlCompleted({
                resourceId: sourceId,
                status: 'completed',
                progress: 60,
                message: `ƒê√£ t·∫°o ${chunks.length} chunks. B·∫Øt ƒë·∫ßu embedding...`
            })

            // Create embeddings
            console.log('üß† Starting embedding process...')
            this.chatbotGateway.emitEmbeddingProgress({
                resourceId: sourceId,
                status: 'embedding',
                progress: 60,
                message: `ƒêang t·∫°o embedding cho ${chunks.length} chunks...`
            })

            const knowledgeChunks: CreateKnowledgeChunkDto[] = []
            let processedChunks = 0

            for (const chunk of chunks) {
                const vector = await this.getEmbeddingProvider.getEmbedding(chunk)
                knowledgeChunks.push({
                    source_id: sourceId,
                    source_type: knowledgeSource.source_type, // L∆∞u source_type
                    text: chunk,
                    plot_embedding_gemini_large: vector
                })

                processedChunks++
                const embeddingProgress = 60 + (processedChunks / chunks.length) * 30

                if (processedChunks % 5 === 0 || processedChunks === chunks.length) {
                    console.log(`üìä Embedding progress: ${processedChunks}/${chunks.length}`)
                    this.chatbotGateway.emitEmbeddingProgress({
                        resourceId: sourceId,
                        status: 'embedding',
                        progress: Math.round(embeddingProgress),
                        message: `ƒê√£ embedding ${processedChunks}/${chunks.length} chunks...`
                    })
                }
            }

            console.log(`üíæ Saving ${knowledgeChunks.length} chunks to database...`)
            console.log('üìã First chunk sample:', {
                source_id: knowledgeChunks[0]?.source_id,
                text_length: knowledgeChunks[0]?.text?.length,
                has_embedding: !!knowledgeChunks[0]?.plot_embedding_gemini_large,
                embedding_length: knowledgeChunks[0]?.plot_embedding_gemini_large?.length
            })

            // Save chunks
            const saveResult = await this.knowledgeChunksProvider.createKnowledgeChunks(knowledgeChunks)
            console.log('‚úÖ Chunks save result:', saveResult)

            // Direct verification using mongoose model
            const KnowledgeChunkModel = this.knowledgeChunksProvider['knowledgeChunkRepository']['knowledgeChunkModel']
            const directCount = await KnowledgeChunkModel.countDocuments({ source_id: sourceId })
            console.log(`üîç Direct count query: Found ${directCount} chunks with source_id: ${sourceId}`)

            if (directCount > 0) {
                const sampleChunks = await KnowledgeChunkModel.find({ source_id: sourceId }).limit(3).lean()
                console.log(
                    'üìã Sample chunk IDs:',
                    sampleChunks.map((c) => c._id.toString())
                )
                console.log(
                    'üìã Sample chunk texts:',
                    sampleChunks.map((c) => c.text.substring(0, 50) + '...')
                )
            } else {
                console.error('‚ö†Ô∏è  WARNING: No chunks found in database after save!')
                console.error('‚ö†Ô∏è  Check if sourceId matches:', sourceId)
            }

            console.log('üìù Updating metadata...')
            // Update metadata and status
            await this.knowledgeSourceProvider.updateKnowledgeSourceMetadata(sourceId, {
                wordCount,
                chunkCount: chunks.length,
                progress: 100
            })

            console.log('‚úÖ Updating status to COMPLETED...')
            await this.knowledgeSourceProvider.updateKnowledgeSourceStatus(sourceId, ProcessingStatus.COMPLETED)

            console.log('üéâ Emitting completion event...')
            this.chatbotGateway.emitEmbeddingCompleted({
                resourceId: sourceId,
                status: 'completed',
                progress: 100,
                message: `Ho√†n th√†nh! ƒê√£ t·∫°o ${chunks.length} chunks t·ª´ ${wordCount} t·ª´.`
            })

            console.log(`‚úÖ loadSampleDataWithProgress completed successfully for sourceId: ${sourceId}`)
        } catch (error) {
            console.error('‚ùå Error in loadSampleDataWithProgress:', error)
            console.error('Error stack:', error.stack)

            await this.knowledgeSourceProvider.updateKnowledgeSourceStatus(sourceId, ProcessingStatus.FAILED)
            await this.knowledgeSourceProvider.updateKnowledgeSourceMetadata(sourceId, {
                errorMessage: error.message
            })

            this.chatbotGateway.emitCrawlFailed({
                resourceId: sourceId,
                status: 'failed',
                progress: 0,
                message: 'Crawl th·∫•t b·∫°i',
                error: error.message
            })

            throw error
        }
    }

    scrapePage = async (url: string) => {
        const loader = new PuppeteerWebBaseLoader(url, {
            launchOptions: {
                headless: true
            },
            gotoOptions: {
                waitUntil: 'domcontentloaded'
            },
            evaluate: async (page, browser) => {
                const result = await page.evaluate(() => {
                    return document.body.innerText
                })
                await browser.close()
                return result
            }
        })
        return (await loader.scrape())?.replace(/<[^>]*>?/gm, '')
    }
}
