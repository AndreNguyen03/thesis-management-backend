import { Injectable, Logger } from '@nestjs/common'

@Injectable()
export class RerankerService {
    private readonly logger = new Logger(RerankerService.name)

    // Vietnamese stopwords ƒë·∫ßy ƒë·ªß
    private readonly VIETNAMESE_STOPWORDS = new Set([
        // Common stopwords
        'v√†',
        'c·ªßa',
        'cho',
        'v·ªõi',
        'l√†',
        'c√°c',
        'm·ªôt',
        'ƒë∆∞·ª£c',
        'trong',
        'khi',
        'v·ªÅ',
        'theo',
        'nh∆∞',
        'n√†y',
        'ƒë√≥',
        'n√†o',
        'c√≥',
        'kh√¥ng',
        't·ª´',
        'ph·∫£i',
        'n√™n',
        'r·∫•t',
        'c≈©ng',
        'ƒë√£',
        's·∫Ω',
        'th√¨',
        'm√†',
        '·ªü',
        'b·ªüi',
        'v√¨',
        'ƒë·ªÉ',
        'n·∫øu',
        'th√¨',
        'hay',
        'ho·∫∑c',
        'v·∫´n',
        'l·∫°i',
        'ra',
        'v√†o',
        'l√™n',
        'xu·ªëng',
        'qua',
        'l·∫°i',
        'ƒë·∫øn',
        't·ª´',
        'tr√™n',
        'd∆∞·ªõi',
        'tr∆∞·ªõc',
        'sau',
        'ngo√†i',

        // Academic stopwords
        'ƒë·ªÅ',
        't√†i',
        'sinh',
        'vi√™n',
        'nghi√™n',
        'c·ª©u',
        'khoa',
        'h·ªçc',
        'h·ªá',
        'th·ªëng',
        '·ª©ng',
        'd·ª•ng',
        'ph√°t',
        'tri·ªÉn',
        'x√¢y',
        'd·ª±ng',
        'qu·∫£n',
        'l√Ω',
        'c√¥ng',
        'ngh·ªá',
        'th√¥ng',
        'tin',
        'tr∆∞·ªùng',
        'ƒë·∫°i',
        'h·ªçc',
        'gi·∫£ng',
        'vi√™n',
        'b√†i',
        't·∫≠p',
        'ƒë·ªì',
        '√°n',
        'lu·∫≠n',
        'vƒÉn',
        'kh√≥a',
        'lu·∫≠n',
        't·ªët',
        'nghi·ªáp',

        // General stopwords
        't√¥i',
        'b·∫°n',
        'anh',
        'ch·ªã',
        'em',
        'ch√∫ng',
        'ta',
        'h·ªç',
        'm√¨nh',
        'g√¨',
        'n√†o',
        'sao',
        'bao',
        'gi·ªù',
        'ƒë√¢u',
        'n∆°i',
        'th·∫ø',
        'n√†o',
        't·∫°i',
        'sao',

        // Articles
        'm·ªôt',
        'nh·ªØng',
        'c√°c',
        'm·∫•y',
        'v√†i'
    ])

    // Domain keywords v·ªõi weights (1.0-2.0)
    private readonly DOMAIN_KEYWORDS = new Map([
        // AI/ML (High priority)
        ['ai', 2.0],
        ['machine learning', 2.0],
        ['deep learning', 2.0],
        ['artificial intelligence', 2.0],
        ['neural network', 1.8],
        ['computer vision', 1.8],
        ['natural language processing', 1.8],
        ['nlp', 1.8],
        ['data mining', 1.7],
        ['reinforcement learning', 1.7],

        // Web Development
        ['web', 1.5],
        ['website', 1.5],
        ['frontend', 1.6],
        ['backend', 1.6],
        ['fullstack', 1.7],
        ['react', 1.8],
        ['vue', 1.8],
        ['angular', 1.8],
        ['nextjs', 1.7],
        ['nodejs', 1.8],
        ['express', 1.7],
        ['nestjs', 1.7],
        ['javascript', 1.7],
        ['typescript', 1.7],
        ['html', 1.3],
        ['css', 1.3],

        // Mobile
        ['mobile', 1.5],
        ['android', 1.8],
        ['ios', 1.8],
        ['flutter', 1.8],
        ['react native', 1.8],
        ['kotlin', 1.7],
        ['swift', 1.7],

        // Database
        ['database', 1.5],
        ['mongodb', 1.7],
        ['mysql', 1.7],
        ['postgresql', 1.7],
        ['sql', 1.6],
        ['nosql', 1.6],
        ['redis', 1.6],

        // Cloud & DevOps
        ['cloud', 1.6],
        ['aws', 1.8],
        ['azure', 1.8],
        ['gcp', 1.8],
        ['docker', 1.7],
        ['kubernetes', 1.7],
        ['devops', 1.7],
        ['ci/cd', 1.6],

        // Data Science
        ['data science', 1.8],
        ['data analysis', 1.7],
        ['big data', 1.7],
        ['data visualization', 1.6],
        ['statistics', 1.5],

        // Research & Academic
        ['research', 1.4],
        ['algorithm', 1.6],
        ['model', 1.5],
        ['analysis', 1.5],
        ['evaluation', 1.4],
        ['survey', 1.3],
        ['experiment', 1.4],
        ['methodology', 1.4],

        // Security
        ['security', 1.6],
        ['cybersecurity', 1.7],
        ['encryption', 1.6],
        ['authentication', 1.5],

        // Vietnamese specific tech terms
        ['x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n', 1.8],
        ['h·ªçc m√°y', 2.0],
        ['h·ªçc s√¢u', 2.0],
        ['tr√≠ tu·ªá nh√¢n t·∫°o', 2.0],
        ['ph√¢n t√≠ch d·ªØ li·ªáu', 1.7],
        ['khoa h·ªçc d·ªØ li·ªáu', 1.8],
        ['.net', 1.8],
        ['c#', 1.8],
        ['c sharp', 1.8],
        ['aws', 1.8],
        ['c++', 1.7],
        ['git', 1.5],
        ['java', 1.8],
        ['javascript', 1.8],
        ['mongodb', 1.8],
        ['nodejs', 1.8],
        ['node.js', 1.8],
        ['sql', 1.7],
        ['reactjs', 1.8],
        ['react', 1.8],
        ['nestjs', 1.8],
        ['postgresql', 1.8],

        // Th√™m c√°c interests mapping
        ['devops', 2.0],
        ['t·ª± ƒë·ªông h√≥a', 1.8],
        ['tri·ªÉn khai', 1.7],
        ['tr√≠ tu·ªá nh√¢n t·∫°o', 2.0],
        ['ai', 2.0],
        ['h·ªçc m√°y', 2.0],
        ['machine learning', 2.0],
        ['microservices', 1.9],
        ['ki·∫øn tr√∫c microservices', 2.0],
        ['h·ªá th·ªëng ph√¢n t√°n', 1.8],
        ['t·ªëi ∆∞u hi·ªáu nƒÉng', 1.7],
        ['ph√°t tri·ªÉn web', 1.8],
        ['·ª©ng d·ª•ng web', 1.8]
    ])

    /**
     * Rerank topics based on lexical overlap
     */
    async rerank(
        topicSummaries: string[],
        studentSummary: string
    ): Promise<{
        scores: number[]
        matchedKeywords: string[][]
        explanations: string[]
    }> {
        const studentTokens = this.tokenize(studentSummary)

        const results = topicSummaries.map((topicSummary, index) => {
            const topicTokens = this.tokenize(topicSummary)

            // 1. Jaccard similarity
            const jaccardScore = this.calculateJaccardSimilarity(studentTokens, topicTokens)

            // 2. Keyword overlap with weights
            const keywordResult = this.calculateKeywordOverlap(studentTokens, topicTokens)

            // 3. N-gram overlap (bigrams and trigrams)
            const bigramScore = this.calculateNgramOverlap(studentSummary, topicSummary, 2)
            const trigramScore = this.calculateNgramOverlap(studentSummary, topicSummary, 3)

            // 4. Phrase matching score
            const phraseScore = this.calculatePhraseOverlap(studentSummary, topicSummary)

            // 5. Combine scores
            const baseScore = this.combineScores(jaccardScore, keywordResult.score, bigramScore, trigramScore)

            // Boost score if phrase matching is good
            const finalScore = baseScore + phraseScore * 0.2

            // 6. Generate explanation
            const explanation = this.generateExplanation(
                finalScore,
                keywordResult.matchedKeywords,
                jaccardScore,
                phraseScore
            )

            this.logDebug(index, jaccardScore, keywordResult.score, bigramScore, trigramScore, finalScore, phraseScore)

            return {
                score: Math.min(finalScore, 1.0),
                matchedKeywords: keywordResult.matchedKeywords,
                explanation
            }
        })

        return {
            scores: results.map((r) => r.score),
            matchedKeywords: results.map((r) => r.matchedKeywords),
            explanations: results.map((r) => r.explanation)
        }
    }

    /**
     * Tokenize text with Vietnamese support
     */
    private tokenize(text: string): Set<string> {
        if (!text || text.trim().length === 0) {
            return new Set()
        }

        return new Set(
            text
                .toLowerCase()
                // Normalize Vietnamese accents
                .normalize('NFD')
                .replace(/[\u0300-\u036f]/g, '')
                // Split by whitespace and punctuation
                .split(/[\s\p{P}]+/u)
                .map((word) => word.trim())
                .filter((word) => word.length > 2 && !this.VIETNAMESE_STOPWORDS.has(word) && this.isValidToken(word))
            // Remove duplicates
        )
    }

    /**
     * Check if token is valid (contains letters/numbers)
     */
    private isValidToken(word: string): boolean {
        return /[\p{L}0-9]/u.test(word)
    }

    /**
     * Calculate Jaccard similarity
     */
    private calculateJaccardSimilarity(setA: Set<string>, setB: Set<string>): number {
        if (setA.size === 0 || setB.size === 0) {
            return 0
        }

        const intersection = [...setA].filter((x) => setB.has(x)).length
        const union = setA.size + setB.size - intersection

        return union > 0 ? intersection / union : 0
    }

    /**
     * Calculate keyword overlap v·ªõi domain weights
     */
    private calculateKeywordOverlap(
        studentTokens: Set<string>,
        topicTokens: Set<string>
    ): { score: number; matchedKeywords: string[] } {
        const matchedKeywords: string[] = []
        let totalWeight = 0
        let maxPossibleWeight = 0

        // Convert to arrays for easier processing
        const studentArray = [...studentTokens]
        const topicArray = [...topicTokens]

        // Check each domain keyword
        for (const [keyword, weight] of this.DOMAIN_KEYWORDS.entries()) {
            // Check if keyword exists in student profile
            const studentHasKeyword = studentArray.some((token) => this.isKeywordMatch(token, keyword))

            // Check if keyword exists in topic
            const topicHasKeyword = topicArray.some((token) => this.isKeywordMatch(token, keyword))

            if (studentHasKeyword && topicHasKeyword) {
                matchedKeywords.push(keyword)
                totalWeight += weight
            }

            maxPossibleWeight += weight
        }

        // Calculate normalized score
        const score = maxPossibleWeight > 0 ? totalWeight / maxPossibleWeight : 0

        return {
            score: Math.min(score, 1),
            matchedKeywords
        }
    }

    /**
     * Check if token matches keyword (allows partial matches)
     */
    private isKeywordMatch(token: string, keyword: string): boolean {
        // Normalize both
        const normalizedToken = token.toLowerCase().replace(/[^a-z0-9]/g, '')
        const normalizedKeyword = keyword.toLowerCase().replace(/[^a-z0-9\s]/g, '')

        // Check exact match after normalization
        if (normalizedToken === normalizedKeyword.replace(/\s+/g, '')) {
            return true
        }

        // For multi-word keywords
        if (normalizedKeyword.includes(' ')) {
            // Option 1: All parts must be present (strict)
            const keywordParts = normalizedKeyword.split(/\s+/)
            const allPartsPresent = keywordParts.every(
                (part) => normalizedToken.includes(part) || normalizedToken.replace(part, '') !== normalizedToken
            )

            if (allPartsPresent) return true

            // Option 2: Phrase matching
            const keywordWithoutSpaces = normalizedKeyword.replace(/\s+/g, '')
            if (normalizedToken.includes(keywordWithoutSpaces)) {
                return true
            }
        }

        // Partial match for compound words
        if (normalizedKeyword.length > 3 && normalizedToken.length > 3) {
            // Check if one is substring of another (with threshold)
            const minLength = Math.min(normalizedToken.length, normalizedKeyword.length)
            const maxLength = Math.max(normalizedToken.length, normalizedKeyword.length)

            if (minLength / maxLength > 0.7) {
                // 70% similarity
                return normalizedToken.includes(normalizedKeyword) || normalizedKeyword.includes(normalizedToken)
            }
        }

        return false
    }

    /**
     * Add trigram overlap
     */
    private calculateTrigramOverlap(textA: string, textB: string): number {
        return this.calculateNgramOverlap(textA, textB, 3)
    }

    /**
     * Improved score combination
     */
    private combineScores(
        jaccardScore: number,
        keywordScore: number,
        bigramScore: number,
        trigramScore: number = 0
    ): number {
        // Dynamic weights based on score quality
        let jaccardWeight = 0.2
        let keywordWeight = 0.5
        let bigramWeight = 0.15
        let trigramWeight = 0.15

        // If keyword score is high, give it more weight
        if (keywordScore > 0.3) {
            keywordWeight = 0.6
            jaccardWeight = 0.15
        }

        // If n-gram scores are more reliable
        const ngramReliability = (bigramScore + trigramScore) / 2
        if (ngramReliability > 0.4) {
            bigramWeight = 0.2
            trigramWeight = 0.2
            keywordWeight = 0.4
        }

        const weightedScore =
            jaccardScore * jaccardWeight +
            keywordScore * keywordWeight +
            bigramScore * bigramWeight +
            trigramScore * trigramWeight

        return this.applyNonLinearScaling(weightedScore)
    }

    /**
     * Calculate n-gram overlap
     */
    private calculateNgramOverlap(textA: string, textB: string, n: number): number {
        const ngramsA = this.extractNgrams(textA, n)
        const ngramsB = this.extractNgrams(textB, n)

        if (ngramsA.size === 0 || ngramsB.size === 0) {
            return 0
        }

        const intersection = [...ngramsA].filter((x) => ngramsB.has(x)).length
        const union = ngramsA.size + ngramsB.size - intersection

        return union > 0 ? intersection / union : 0
    }

    /**
     * Extract n-grams from text
     */
    private extractNgrams(text: string, n: number): Set<string> {
        const ngrams = new Set<string>()
        const tokens = [...this.tokenize(text)] // Convert to array

        for (let i = 0; i <= tokens.length - n; i++) {
            const ngram = tokens.slice(i, i + n).join(' ')
            ngrams.add(ngram)
        }

        return ngrams
    }

    /**
     * Apply non-linear scaling to emphasize good matches
     */
    private applyNonLinearScaling(score: number): number {
        if (score < 0.2) {
            return score * 0.5 // Suppress very low scores
        } else if (score < 0.5) {
            return score * 1.0 // Keep as is
        } else if (score < 0.8) {
            return score * 1.2 // Boost medium scores
        } else {
            return Math.pow(score, 1.3) // Strong emphasis on high scores
        }
    }

    /**
     * Generate human-readable explanation
     */
    private logDebug(
        index: number,
        jaccardScore: number,
        keywordScore: number,
        bigramScore: number,
        trigramScore: number,
        phraseScore: number,
        finalScore: number
    ): void {
        this.logger.debug({
            event: 'topic_reranked',
            topicIndex: index,
            timestamp: new Date().toISOString(),
            scores: {
                jaccard: parseFloat(jaccardScore.toFixed(3)),
                keyword: parseFloat(keywordScore.toFixed(3)),
                bigram: parseFloat(bigramScore.toFixed(3)),
                trigram: parseFloat(trigramScore.toFixed(3)),
                phrase: parseFloat(phraseScore.toFixed(3)),
                final: parseFloat(finalScore.toFixed(3))
            },
            scoreComponents: {
                jaccardPercent: `${(jaccardScore * 100).toFixed(1)}%`,
                keywordPercent: `${(keywordScore * 100).toFixed(1)}%`,
                bigramPercent: `${(bigramScore * 100).toFixed(1)}%`,
                trigramPercent: `${(trigramScore * 100).toFixed(1)}%`,
                phrasePercent: `${(phraseScore * 100).toFixed(1)}%`
            },
            interpretation: this.getScoreInterpretation(finalScore)
        })
    }

    private generateExplanation(
        score: number,
        matchedKeywords: string[],
        jaccardScore: number,
        phraseScore: number
    ): string {
        const scorePercent = (score * 100).toFixed(1)
        const keywordCount = matchedKeywords.length

        // T·∫°o description chi ti·∫øt h∆°n
        let baseExplanation = ''
        let details: string[] = []

        if (score >= 0.8) {
            baseExplanation = 'üî• Tuy·ªát v·ªùi! ƒê·ªô t∆∞∆°ng ƒë·ªìng t·ª´ kh√≥a r·∫•t cao'
            if (keywordCount >= 4) {
                details.push(`${keywordCount} t·ª´ kh√≥a quan tr·ªçng tr√πng kh·ªõp`)
            }
            if (jaccardScore >= 0.6) {
                details.push(`ƒê·ªô t∆∞∆°ng ƒë·ªìng t·ªïng th·ªÉ cao (${(jaccardScore * 100).toFixed(0)}%)`)
            }
            if (phraseScore >= 0.5) {
                details.push(`C·ª•m t·ª´ chuy√™n ng√†nh tr√πng kh·ªõp t·ªët`)
            }
        } else if (score >= 0.7) {
            baseExplanation = '‚úÖ R·∫•t t·ªët! T·ª´ kh√≥a t∆∞∆°ng ƒë·ªìng cao'
            if (keywordCount >= 3) {
                details.push(`${keywordCount} t·ª´ kh√≥a ch√≠nh tr√πng kh·ªõp`)
            }
            if (jaccardScore >= 0.5) {
                details.push(`N·ªôi dung c√≥ nhi·ªÅu ƒëi·ªÉm chung`)
            }
        } else if (score >= 0.6) {
            baseExplanation = 'üëç T·ªët! Ph√π h·ª£p v·ªõi h·ªì s∆°'
            if (keywordCount >= 2) {
                details.push(`${keywordCount} t·ª´ kh√≥a quan tr·ªçng ph√π h·ª£p`)
            }
            if (phraseScore >= 0.3) {
                details.push(`C√≥ c·ª•m t·ª´ chuy√™n m√¥n t∆∞∆°ng ƒë·ªìng`)
            }
        } else if (score >= 0.5) {
            baseExplanation = 'üìä Kh√° ph√π h·ª£p'
            if (keywordCount > 0) {
                details.push(`${keywordCount} t·ª´ kh√≥a ph√π h·ª£p`)
            } else if (jaccardScore >= 0.4) {
                details.push(`N·ªôi dung c√≥ m·ªôt s·ªë ƒëi·ªÉm chung`)
            }
            details.push(`C√≥ th·ªÉ xem x√©t th√™m`)
        } else if (score >= 0.4) {
            baseExplanation = '‚ÑπÔ∏è C√≥ th·ªÉ tham kh·∫£o'
            if (keywordCount > 0) {
                details.push(`C√≥ ${keywordCount} t·ª´ kh√≥a li√™n quan`)
            }
            details.push(`M·ª©c ƒë·ªô ph√π h·ª£p trung b√¨nh`)
        } else if (score >= 0.3) {
            baseExplanation = 'üëÄ H·∫°n ch·∫ø'
            if (keywordCount > 0) {
                details.push(`Ch·ªâ c√≥ ${keywordCount} t·ª´ kh√≥a li√™n quan`)
            }
            details.push(`ƒê·ªô t∆∞∆°ng ƒë·ªìng th·∫•p`)
        } else if (score >= 0.2) {
            baseExplanation = '‚ö†Ô∏è √çt li√™n quan'
            details.push(`R·∫•t √≠t t·ª´ kh√≥a tr√πng kh·ªõp`)
            details.push(`C·∫ßn xem x√©t k·ªπ y√™u c·∫ßu ƒë·ªÅ t√†i`)
        } else if (score >= 0.1) {
            baseExplanation = 'üîç H·∫ßu nh∆∞ kh√¥ng li√™n quan'
            details.push(`Kh√¥ng c√≥ t·ª´ kh√≥a quan tr·ªçng tr√πng kh·ªõp`)
        } else {
            baseExplanation = 'üö´ Kh√¥ng ph√π h·ª£p'
            details.push(`H·ªì s∆° v√† ƒë·ªÅ t√†i kh√¥ng c√≥ ƒëi·ªÉm chung`)
        }

        // Th√™m th√¥ng tin ch·∫•t l∆∞·ª£ng matching
        if (keywordCount > 0) {
            const importantKeywords = matchedKeywords.filter((kw) => {
                const keywordWeight = this.DOMAIN_KEYWORDS.get(kw)
                // Check both: keyword exists AND weight >= 1.7
                return keywordWeight !== undefined && keywordWeight >= 1.7
            })

            if (importantKeywords.length > 0) {
                details.push(
                    `C√≥ ${importantKeywords.length} t·ª´ kh√≥a quan tr·ªçng: ${importantKeywords.slice(0, 3).join(', ')}${importantKeywords.length > 3 ? '...' : ''}`
                )
            }
        }

        // Gh√©p explanation
        let explanation = `${baseExplanation} (${scorePercent}%)`
        if (details.length > 0) {
            explanation += `. ${details.join('. ')}.`
        }

        // Th√™m g·ª£i √Ω d·ª±a tr√™n score
        if (score < 0.5) {
            explanation += ` G·ª£i √Ω: Ki·ªÉm tra y√™u c·∫ßu k·ªπ thu·∫≠t c·ª• th·ªÉ c·ªßa ƒë·ªÅ t√†i.`
        } else if (score < 0.7) {
            explanation += ` G·ª£i √Ω: Xem x√©t m√¥ t·∫£ chi ti·∫øt ƒë·ªÉ ƒë√°nh gi√° ph√π h·ª£p.`
        }

        return explanation
    }

    private getScoreInterpretation(score: number): string {
        if (score >= 0.8) return 'excellent_match'
        if (score >= 0.7) return 'very_good_match'
        if (score >= 0.6) return 'good_match'
        if (score >= 0.5) return 'acceptable_match'
        if (score >= 0.4) return 'below_average_match'
        if (score >= 0.3) return 'poor_match'
        if (score >= 0.2) return 'very_poor_match'
        return 'no_match'
    }

    /**
     * Get all domain keywords (for debugging)
     */
    getDomainKeywords(): Map<string, number> {
        return this.DOMAIN_KEYWORDS
    }

    /**
     * Get stopwords count (for debugging)
     */
    getStopwordsCount(): number {
        return this.VIETNAMESE_STOPWORDS.size
    }

    private calculatePhraseOverlap(textA: string, textB: string): number {
        const phrasesA = this.extractPhrases(textA)
        const phrasesB = this.extractPhrases(textB)

        if (phrasesA.size === 0 || phrasesB.size === 0) return 0

        let matches = 0
        for (const phraseA of phrasesA) {
            for (const phraseB of phrasesB) {
                if (this.similarPhrases(phraseA, phraseB)) {
                    matches++
                    break
                }
            }
        }

        return matches / Math.max(phrasesA.size, phrasesB.size)
    }

    /**
     * Extract meaningful phrases (3-5 words)
     */
    private extractPhrases(text: string): Set<string> {
        const phrases = new Set<string>()
        const tokens = [...this.tokenize(text)]

        // Extract 3-5 word phrases
        for (let n = 3; n <= 5; n++) {
            for (let i = 0; i <= tokens.length - n; i++) {
                const phrase = tokens.slice(i, i + n).join(' ')
                if (phrase.length >= 10) {
                    // Minimum length
                    phrases.add(phrase)
                }
            }
        }

        return phrases
    }

    /**
     * Check if two phrases are similar
     */
    private similarPhrases(phraseA: string, phraseB: string): boolean {
        const a = phraseA.toLowerCase()
        const b = phraseB.toLowerCase()

        // Exact match
        if (a === b) return true

        // One contains the other
        if (a.includes(b) || b.includes(a)) return true

        // Jaccard similarity for phrases
        const wordsA = new Set(a.split(' '))
        const wordsB = new Set(b.split(' '))
        const intersection = [...wordsA].filter((x) => wordsB.has(x)).length
        const union = wordsA.size + wordsB.size - intersection

        return intersection / union > 0.6 // 60% overlap
    }

    /**
     * Normalize DOMAIN_KEYWORDS
     */
    private normalizeDomainKeywords(): void {
        // Remove duplicates and standardize
        const normalized = new Map<string, number>()

        for (const [keyword, weight] of this.DOMAIN_KEYWORDS.entries()) {
            const normalizedKey = keyword
                .toLowerCase()
                .replace(/[^a-z0-9\s]/g, '')
                .replace(/\s+/g, ' ')
                .trim()

            // Keep the highest weight if duplicate
            if (!normalized.has(normalizedKey) || normalized.get(normalizedKey)! < weight) {
                normalized.set(normalizedKey, weight)
            }
        }

        // Update DOMAIN_KEYWORDS
        this.DOMAIN_KEYWORDS.clear()
        normalized.forEach((weight, key) => this.DOMAIN_KEYWORDS.set(key, weight))
    }

    /**
     * Initialize with normalized keywords
     */
    constructor() {
        this.normalizeDomainKeywords()
    }

    /**
     * Test tokenization
     */
    testTokenization(text: string): {
        original: string
        tokens: string[]
        filteredTokens: string[]
        removedStopwords: string[]
    } {
        const allTokens = text
            .toLowerCase()
            .normalize('NFD')
            .replace(/[\u0300-\u036f]/g, '')
            .split(/[\s\p{P}]+/u)
            .map((w) => w.trim())
            .filter((w) => w.length > 0)

        const filteredTokens = allTokens.filter(
            (w) => w.length > 2 && !this.VIETNAMESE_STOPWORDS.has(w) && this.isValidToken(w)
        )

        const removedStopwords = allTokens.filter((w) => this.VIETNAMESE_STOPWORDS.has(w) || w.length <= 2)

        return {
            original: text,
            tokens: allTokens,
            filteredTokens,
            removedStopwords
        }
    }
}
